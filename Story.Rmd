---
title: "Analysis of AirBnB's in Zurich City"
author: "Ana Mas Urquijo, Azeglio Martinelli"
date: "`r Sys.Date()`"
output:
  html_document:
    highlight: tango
    number_sections: true
    toc: true               # table of contents
    toc_depth: 3            # heading levels displayed
    toc_float:
      collapsed: false      # toc in panel on the left
      smooth_scroll: true   # scroll instead of jump
      
    
```{r knitr-setup, include = FALSE}
library(knitr)
opts_chunk$set(echo = FALSE, include = FALSE)
```
---

# Description of Problem

# Source of Data

# Objective and hypothesis of analysis



# Data Preparation

The cell below was run only once to unpack the reviews data from a .gz
compressed folder. The code cell unpacked the data and directly deleted
the compressed files.

```{r, eval=FALSE}
library(R.utils)
gunzip("Data/reviews.csv.gz")
```
First, the listings and reviews datasets are imported from CSV files using read_csv and stored as d.listings and d.reviews. After loading the listings data, the amount of missing data is examined by calculating both the absolute number of missing values in each column and the proportion of missing values relative to the total number of observations. This provides an overview of data completeness and helps identify variables that may require cleaning or removal.

Based on this inspection, the column license is removed from the listings dataset because it contains only missing values and therefore provides no usable information for the analysis. The remaining missing values in selected numeric variables are then addressed by replacing NA values with zeros for price and reviews_per_month, ensuring that these variables contain valid numeric values and can be used in subsequent analyses. The last_review variable is intentionally left unchanged, as imputing a date where none exists would not be meaningful.

The same missing-data diagnostics are then applied to the reviews dataset by again calculating the number and proportion of missing values in each column. Missing values in the comments column are replaced with the string "Unknown" so that all review records contain a valid text entry and can be processed consistently.

Finally, the listings dataset is checked for duplicate listing identifiers. This is done by counting how often each id appears, filtering for those that occur more than once, and displaying only the distinct duplicated entries. This step verifies whether the primary key in the listings data is unique, which is important for reliable joins and further analysis.

```{r}
library(readr)
library(dplyr)
library(tidyr)

d.listings <- read_csv("Data/listings.csv")
d.reviews <- read_csv("Data/reviews.csv")

# missing data in each column
colSums(is.na(d.listings))
# percentage of missing data
colMeans(is.na(d.listings))

# drop column license as it contains only NA values
d.listings <- d.listings %>% 
  select(-license)

# replace NA's in other columns so that we have valid values for analysis (except for last_review because here it does not make sense to enter a somewhat valid date for NA values)
d.listings <- d.listings %>% 
  replace_na(list(price = 0, reviews_per_month = 0))

# same checks for reviews and replace NA's
colSums(is.na(d.reviews))
colMeans(is.na(d.reviews))

d.reviews <- d.reviews %>% 
  replace_na(list(comments = 'Unkown'))

# Check for duplicate Id's in listings
d.listings %>%
  add_count(id) %>%
  filter(n>1) %>%
  distinct()

```

In the next cell below, the two data sets were merged.

First, a join condition is explicitly defined to link the two datasets by matching the id column in the listings data with the listing_id column in the reviews data. Using this condition, the cleaned listings dataset is merged with the reviews dataset using a left join, ensuring that all listings are retained in the merged dataset while review information is added where a matching review exists.

After performing the merge, sanity checks are carried out to assess whether the join behaved as expected. The total number of rows in the merged dataset is computed and compared with the number of rows in the listings data. In addition, the number of unmatched rows is calculated by counting how many entries have missing values in the review identifier column, indicating listings for which no corresponding review record was found.

To further investigate unmatched cases, an anti-join is performed between the listings and reviews datasets. This operation returns only those listings that do not have a matching review. The result is inspected manually using the View() function, allowing a detailed examination of all columns. This inspection confirms that the unmatched entries correspond to listings without any reviews, which explains the absence of a matching listing_id in the reviews data, particularly given that the last_review field contains only missing values for these cases.

Finally, to improve clarity and avoid ambiguity in the merged dataset, review-related columns are renamed. The review identifier column originating from the reviews table is renamed to review_id, and the review date column is renamed to review_date, making the structure and meaning of the merged data clearer for subsequent analysis.


```{r}
by <- join_by(id == listing_id)

d.merged <- d.listings %>%
  left_join(d.reviews, by = by)

# Sanity checks for merge below

# Check number of matches where join columns of reviews are NA
d.merged %>%
  summarise(
    total = n(),
    unmatched = sum(is.na(id.y))
  )

# anti_join to inspect any mismatches
anti_join(listings, reviews, by = c("id" = "listing_id")) |>
  View()

# Upon closer, manual inspection wiht the View() it is evident that all the non matched columns exist because they do not have a review and therefore no listing_id to match (because column last_review is all NA's).

# rename different review columns
d.merged <- d.merged %>% 
  rename(review_id = id.y, review_date = date)

```

# Data Visualization / Story

# Model

# Chapter of choice

# Conclusions

# Potential Limitations

# GenAI

Ideas of questions to answer:

**What have you used it for?**

**For what kind of tasks turned they out to be helpful?**

**How do you check correctness of the answer?**

**What did not work?**

# Requirements

We cross them as soon as fulfilled:

Two or more data sets which need to be joined/merged

**First data set:**

At least few hundred of observations

At least dozen variables

Numeric and categorical variables

Dates ( YYYY-MM-DD) or geographic locations, both are even better

**Second data set:**

Can be simple

Join the two data sets and highlight in your report where you have done
this, showing the line of code where the joining is performed

Ideally different formats and different sources

**Content** Comprehensive analysis which we will present to a hypothetical
client

In depth examination of problem being adressed

PDF or HTML with Rmarkdwon or quarto

Clear and comprehensible to all readers

**Prepare the data for the analysis** Cleaned before they are merged and
analysis begins

You can hide code but highlight in the text where the data is merged

Please explain in words (not as code) what you have done for data
preparation

**Visualize the data appropiately**
Analyse data using summary statistics

Graphs

*Recommendations for plots*
No pie charts

Alpha level to add transparency to points /lines

Not to many stacked bar plots, or not at all (instead maybe line plots)

Show entire range of values, without zooming too much

Same scale on axes

Shall all observations and infos, not just mean and standard deviation

Not too many dimensions in a graph

**Fit the model**
Fit a model and show result + interpet it

Produce graphs (e.g. predictions or residual diagnostics) for your model fits

Possibility: compare several models via CV

**Chapter of choice**
Use new package that was not mentioned in the main part of the course (cant be data
manipulation, visualisation using ggplot2, regular expressions and reporting using Rmarkdown does not count and perform a task that we have not discussed in the course

No new statistical or machine learning method

Method to prepare or display data

**Generative AI**
Quarter to half a page about which tools you have used

What have you used it for? For what kind of tasks turned they out to be
helpful?

How do you check correctness of the answer?

What did not work?

**Sell the story**
Complete, easy to read, thread (what are you doing, why, conclusions, interpretations, overview . . . )

• Source of the data.
• Objectives and hypotheses of the analysis.
• Interpretation of each step.
• Conclusions drawn from the analysis.
• Potential limitations of the study.

**Comments**
Comment a lot

Check coding style

**Additional requirements**
a readme.txt file;

structured folders (e.g. “Data”, “Scripts”, . . . )

the data set analysed

the Rmarkdown file (.Rmd) (or quarto file)

the PDF or HTML output file

not more than 25 pages

